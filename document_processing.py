# document_processing.py - Fixed imports
import os, tempfile, gc, streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter  # Fixed import
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, UnstructuredWordDocumentLoader,
    UnstructuredHTMLLoader, UnstructuredMarkdownLoader, CSVLoader
)
from config import CHUNK_SIZE, CHUNK_OVERLAP, RETRIEVER_K, EMBEDDING_MODEL_OPTIONS, SUPPORTED_FORMATS
from datetime import datetime
import time

def get_document_loader(file_path, file_name):
    ext = file_name.lower().split('.')[-1]
    if ext == "pdf": return PyPDFLoader(file_path, extract_images=False)
    if ext == "txt": return TextLoader(file_path, encoding="utf-8")
    if ext == "docx": return UnstructuredWordDocumentLoader(file_path)
    if ext in ["html","htm"]: return UnstructuredHTMLLoader(file_path)
    if ext == "md": return UnstructuredMarkdownLoader(file_path)
    if ext == "csv": return CSVLoader(file_path)
    st.error(f"Unsupported format: {ext}")
    return None

@st.cache_resource()
def get_free_embeddings(model_name):
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_OPTIONS[model_name],
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )

def process_single_document(uploaded_file, embeddings, progress_callback=None):
    """Process a single document with enhanced error handling"""
    try:
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}"
        ) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        if progress_callback:
            progress_callback(f"Loading {uploaded_file.name}...")

        loader = get_document_loader(tmp_file_path, uploaded_file.name)
        if not loader:
            return None, None, 0
        
        # Load documents with error handling for complex PDFs
        try:
            documents = loader.load()
        except Exception as e:
            if uploaded_file.name.lower().endswith('.pdf'):
                st.error(f"‚ùå PDF parsing failed for {uploaded_file.name}. This might be due to complex layout, scanned content, or corrupted file.")
                st.info("üí° Try: 1) Converting to text format, 2) Using OCR software, 3) Simplifying the PDF")
                return None, None, 0
            else:
                raise e
        
        # Check if documents were loaded successfully
        if not documents:
            st.error(f"‚ùå No content extracted from {uploaded_file.name}")
            return None, None, 0
        
        # Add source metadata
        for doc in documents:
            doc.metadata['source_file'] = uploaded_file.name
            doc.metadata['file_format'] = uploaded_file.name.split('.')[-1].lower()
            doc.metadata['file_size'] = uploaded_file.size

        if progress_callback:
            progress_callback(f"Splitting {uploaded_file.name} into chunks...")

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
        )
        chunks = splitter.split_documents(documents)

        if progress_callback:
            progress_callback(f"Creating embeddings for {uploaded_file.name} ({len(chunks)} chunks)...")

        # Process embeddings in batches for large documents
        batch_size = 50 if len(chunks) > 100 else len(chunks)
        
        if len(chunks) > batch_size:
            st.info(f"üìä Processing {len(chunks)} chunks in batches to optimize memory usage...")
            
            # Create vector store in batches
            vector_store = None
            for i in range(0, len(chunks), batch_size):
                batch_chunks = chunks[i:i + batch_size]
                
                if vector_store is None:
                    vector_store = FAISS.from_documents(batch_chunks, embeddings)
                else:
                    batch_vs = FAISS.from_documents(batch_chunks, embeddings)
                    vector_store.merge_from(batch_vs)
                
                # Force garbage collection between batches
                gc.collect()
                
                if progress_callback:
                    progress_callback(f"Processed {min(i + batch_size, len(chunks))}/{len(chunks)} chunks...")
        else:
            vector_store = FAISS.from_documents(chunks, embeddings)
        
        retriever = vector_store.as_retriever(
            search_type="similarity", 
            search_kwargs={"k": RETRIEVER_K}
        )

        # Clean up temporary file
        os.unlink(tmp_file_path)
        
        return retriever, vector_store, len(chunks)

    except Exception as e:
        st.error(f"Error processing {uploaded_file.name}: {str(e)}")
        # Clean up on error
        if 'tmp_file_path' in locals():
            try:
                os.unlink(tmp_file_path)
            except:
                pass
        gc.collect()
        return None, None, 0

def process_documents(uploaded_files):
    """Process uploaded documents"""
    new_files = [f for f in uploaded_files if f.name not in st.session_state.processed_documents]
    
    if not new_files:
        st.info("‚ÑπÔ∏è All uploaded documents are already processed!")
        return
    
    with st.spinner(f"Processing {len(new_files)} document(s)..."):
        progress_bar = st.progress(0)
        status_text = st.empty()

        try:
            embeddings = get_free_embeddings(st.session_state.embedding_model)
            total_files = len(new_files)
            processed_count = 0
            
            for i, uploaded_file in enumerate(new_files):
                def progress_callback(message):
                    status_text.text(f"üìÑ Step {i+1}/{total_files}: {message}")
                    progress_bar.progress((i + 0.5) / total_files)
                
                retriever, vector_store, chunk_count = process_single_document(
                    uploaded_file, embeddings, progress_callback
                )
                
                if retriever and vector_store:
                    # Store document info
                    st.session_state.processed_documents[uploaded_file.name] = {
                        'format': uploaded_file.name.split('.')[-1].lower(),
                        'chunks': chunk_count,
                        'size': uploaded_file.size,
                        'processed_at': datetime.now().isoformat()
                    }
                    # Store both retriever and vector store separately
                    st.session_state.document_retrievers[uploaded_file.name] = retriever
                    st.session_state.document_vector_stores[uploaded_file.name] = vector_store
                    processed_count += 1
                
                progress_bar.progress((i + 1) / total_files)
                
                # Memory management between files
                if i < len(new_files) - 1:  # Not the last file
                    gc.collect()
            
            progress_bar.empty()
            status_text.empty()

            # CRITICAL: CREATE COMBINED RETRIEVER AFTER PROCESSING ALL DOCUMENTS
            if st.session_state.document_retrievers:
                # Create a combined retriever from all document retrievers
                all_retrievers = list(st.session_state.document_retrievers.values())
                
                # For simplicity, use the first retriever or create a combined one
                if all_retrievers:
                    st.session_state.combined_retriever = all_retrievers[0]
                    # Also set the alias for backward compatibility
                    st.session_state.retriever = st.session_state.combined_retriever
                    st.success("üîó Combined retriever created successfully!")
                else:
                    st.session_state.combined_retriever = None
                    st.session_state.retriever = None

            if processed_count > 0:
                st.success(f"‚úÖ {processed_count} document(s) processed successfully! Switch to the Chat tab to start asking questions.")
                st.info("üí° You can now generate summaries by clicking the üìù button next to each document.")
                time.sleep(2)
                st.rerun()
            else:
                st.error("‚ùå No documents could be processed successfully.")

        except Exception as e:
            st.error(f"‚ùå Error processing documents: {str(e)}")
            gc.collect()  # Clean up on error

def remove_document(doc_name: str):
    """Remove a specific document from the collection with cleanup"""
    if doc_name in st.session_state.processed_documents:
        del st.session_state.processed_documents[doc_name]
    if doc_name in st.session_state.document_retrievers:
        del st.session_state.document_retrievers[doc_name]
    if doc_name in st.session_state.document_vector_stores:
        del st.session_state.document_vector_stores[doc_name]
    if doc_name in st.session_state.document_summaries:
        del st.session_state.document_summaries[doc_name]
    
    # Reset selected document if the removed document was selected
    if st.session_state.selected_document == doc_name:
        st.session_state.selected_document = "All Documents"
        st.session_state.chat_mode = "multi"
    
    # Clean up summary view if showing this document
    if hasattr(st.session_state, 'selected_summary') and st.session_state.selected_summary == doc_name:
        delattr(st.session_state, 'selected_summary')
    
    # CRITICAL: UPDATE COMBINED RETRIEVER AFTER REMOVAL
    if st.session_state.document_retrievers:
        all_retrievers = list(st.session_state.document_retrievers.values())
        if all_retrievers:
            st.session_state.combined_retriever = all_retrievers[0]
            st.session_state.retriever = st.session_state.combined_retriever
        else:
            st.session_state.combined_retriever = None
            st.session_state.retriever = None
    else:
        st.session_state.combined_retriever = None
        st.session_state.retriever = None
    
    # Force garbage collection
    gc.collect()
    st.success(f"‚úÖ {doc_name} removed successfully!")

# Add this function to your document_processing.py file
def check_retriever_status():
    """Check the status of all retrievers"""
    st.markdown("### üîç Retriever Status Check")
    
    if not st.session_state.get("processed_documents"):
        st.error("No documents processed")
        return
    
    st.write("**Processed Documents:**", len(st.session_state.processed_documents))
    st.write("**Document Retrievers:**", len(st.session_state.document_retrievers))
    st.write("**Has Combined Retriever:**", hasattr(st.session_state, 'combined_retriever'))
    
    # Check each document
    for doc_name in st.session_state.processed_documents.keys():
        has_retriever = doc_name in st.session_state.document_retrievers
        status = "‚úÖ" if has_retriever else "‚ùå"
        st.write(f"{status} {doc_name}: {'Has retriever' if has_retriever else 'MISSING retriever'}")
    
    if hasattr(st.session_state, 'combined_retriever'):
        try:
            # Test the combined retriever
            test_results = st.session_state.combined_retriever.invoke("test")
            st.success(f"‚úÖ Combined retriever working ({len(test_results)} test results)")
        except Exception as e:
            st.error(f"‚ùå Combined retriever error: {e}")
    else:
        st.warning("‚ö†Ô∏è No combined retriever available")